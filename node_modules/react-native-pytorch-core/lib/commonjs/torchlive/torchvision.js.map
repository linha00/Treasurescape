{"version":3,"sources":["torchvision.ts"],"names":["torchvision","__torchlive__"],"mappings":";;;;;;;AAAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAIA;AACA;;AAUA;AACA;AACA;AACA;AACA;AACA;AA4EO,MAAMA,WAAwB,GAAGC,aAAa,CAACD,WAA/C","sourcesContent":["/**\n * Copyright (c) Meta Platforms, Inc. and affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n *\n * @format\n */\n\nimport type {Tensor} from './torch';\n\n// The TransformFn and TransformForwardFn provide both API interfaces to the\n// developer: `transform(tensor)` and `transform.forward(tensor)`.\ntype TransformFn = (tensor: Tensor) => Tensor;\ntype TransformForwardFn = {\n  forward: TransformFn;\n};\n\nexport type Transform = TransformFn & TransformForwardFn;\n\ntype InterpolationMode = 'bilinear' | 'nearest' | 'bicubic';\n\n/**\n * Transforms are common image transformations available in the\n * torchvision.transforms module.\n *\n * {@link https://pytorch.org/vision/0.12/transforms.html}\n */\nexport interface Transforms {\n  /**\n   * Crops the image Tensor at the center. It is expected to have `[…, H, W]`\n   * shape, where `…` means an arbitrary number of leading dimensions. If image\n   * size is smaller than output size along any edge, image is padded with 0\n   * and then center cropped.\n   *\n   * {@link https://pytorch.org/vision/0.12/generated/torchvision.transforms.CenterCrop.html}\n   *\n   * @param size Desired output size of the crop. If size is an int instead of\n   * sequence like `(h, w)`, a square crop `(size, size)` is made. If provided\n   * a sequence of length 1, it will be interpreted as `(size[0], size[0])`.\n   */\n  centerCrop(size: number | [number] | [number, number]): Transform;\n\n  /**\n   * Convert image to grayscale. It is expected to have […, 3, H, W] shape,\n   * where … means an arbitrary number of leading dimensions.\n   *\n   * {@link https://pytorch.org/vision/0.12/generated/torchvision.transforms.Grayscale.html}\n   *\n   * @param numOutputChannels Number of channels desired for output image.\n   */\n  grayscale(numOutputChannels?: 1 | 3): Transform;\n\n  /**\n   * Normalize a tensor image with mean and standard deviation. Given mean:\n   * `(mean[1],...,mean[n])` and std: `(std[1],..,std[n])` for `n` channels,\n   * this transform will normalize each channel of the input torch.\n   *\n   * Tensor i.e., `output[channel] = (input[channel] - mean[channel]) / std[channel]`.\n   *\n   * {@link https://pytorch.org/vision/0.12/generated/torchvision.transforms.Normalize.html}\n   *\n   * @param mean Sequence of means for each channel.\n   * @param std Sequence of standard deviations for each channel.\n   * @param inplace Bool to make this operation in-place.\n   */\n  normalize(mean: number[], std: number[], inplace?: boolean): Transform;\n\n  /**\n   * Resize the input tensor image to the given size. It is expected to have\n   * `[…, H, W]` shape, where `…` means an arbitrary number of leading\n   * dimensions.\n   *\n   * {@link https://pytorch.org/vision/0.12/generated/torchvision.transforms.Resize.html}\n   *\n   * @param size Desired output size. If size is a sequence like `(h, w)`,\n   * output size will be matched to this. If size is an int, smaller edge of\n   * the image will be matched to this number. i.e, if `height > width`, then\n   * image will be rescaled to `(size * height / width, size)`.\n   * @param interpolation Desired interpolation enum.\n   * @param maxSize The maximum allowed for the longer edge of the resized\n   * image.\n   * @param antialias Antialias flag. The flag is false by default and can be\n   * set to true for InterpolationMode.BILINEAR only mode.\n   */\n  resize(\n    size: number | [number] | [number, number],\n    interpolation?: InterpolationMode,\n    maxSize?: number,\n    antialias?: boolean,\n  ): Transform;\n}\n\ninterface Torchvision {\n  transforms: Transforms;\n}\n\ntype Torchlive = {\n  torchvision: Torchvision;\n};\n\ndeclare const __torchlive__: Torchlive;\n\nexport const torchvision: Torchvision = __torchlive__.torchvision;\n"]}