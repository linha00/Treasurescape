{"version":3,"sources":["WordpieceTokenizer.ts"],"names":["WordPieceTokenizer","constructor","vocab","unknownToken","neverSplit","lowercase","Map","loadVocabFromString","basicTokenizer","BasicTokenizer","arr","split","forEach","element","index","tokenIdMap","set","idTokenMap","unknownTokenId","get","undefined","Error","tokenize","text","tokens","words","word","has","push","toLowerCase","isBad","subTokens","start","length","end","curSubstr","substr","slice","tokenToId","token","idToToken","tokenId","encode","join","map","decode","tokenIds","texts","replace"],"mappings":";;;;;;;AASA;;;;AASO,MAAMA,kBAAN,CAAyB;AAS9B;AACF;AACA;AACA;AACA;AACEC,EAAAA,WAAW,CAAC;AACVC,IAAAA,KADU;AAEVC,IAAAA,YAAY,GAAG,OAFL;AAGVC,IAAAA,UAAU,GAAG,CAAC,OAAD,EAAU,OAAV,EAAmB,OAAnB,EAA4B,OAA5B,EAAqC,QAArC,CAHH;AAIVC,IAAAA,SAAS,GAAG;AAJF,GAAD,EAKkB;AAAA,wCAlBR,IAAIC,GAAJ,EAkBQ;;AAAA,wCAjBR,IAAIA,GAAJ,EAiBQ;;AAAA;;AAAA,4CAfI,CAAC,CAeL;;AAAA;;AAAA;;AAAA;;AAC3B,SAAKH,YAAL,GAAoBA,YAApB;AACA,SAAKI,mBAAL,CAAyBL,KAAzB;AACA,SAAKG,SAAL,GAAiBA,SAAjB;AACA,SAAKD,UAAL,GAAkB,CAAC,GAAGA,UAAJ,CAAlB;AACA,SAAKI,cAAL,GAAsB,IAAIC,8BAAJ,CAAmB;AACvCL,MAAAA,UAAU,EAAE,KAAKA,UADsB;AAEvCC,MAAAA,SAAS,EAAE,KAAKA;AAFuB,KAAnB,CAAtB;AAID;;AAEOE,EAAAA,mBAAmB,CAACL,KAAD,EAAsB;AAC/C,UAAMQ,GAAG,GAAGR,KAAK,CAACS,KAAN,CAAY,IAAZ,CAAZ;AACAD,IAAAA,GAAG,CAACE,OAAJ,CAAY,CAACC,OAAD,EAAkBC,KAAlB,KAAoC;AAC9C,WAAKC,UAAL,CAAgBC,GAAhB,CAAoBH,OAApB,EAA6BC,KAA7B;AACA,WAAKG,UAAL,CAAgBD,GAAhB,CAAoBF,KAApB,EAA2BD,OAA3B;AACD,KAHD;AAIA,UAAMK,cAAc,GAAG,KAAKH,UAAL,CAAgBI,GAAhB,CAAoB,KAAKhB,YAAzB,CAAvB;;AAEA,QAAIe,cAAc,KAAKE,SAAvB,EAAkC;AAChC,YAAM,IAAIC,KAAJ,CAAU,8CAAV,CAAN;AACD,KAFD,MAEO;AACL,WAAKH,cAAL,GAAsBA,cAAtB;AACD;AACF;AAED;AACF;AACA;AACA;AACA;AACA;AACA;;;AACSI,EAAAA,QAAQ,CAACC,IAAD,EAAyB;AACtC,UAAMC,MAAgB,GAAG,EAAzB;AACA,UAAMC,KAAK,GAAGF,IAAI,CAACZ,KAAL,CAAW,KAAX,CAAd;AAEAc,IAAAA,KAAK,CAACb,OAAN,CAAec,IAAD,IAAkB;AAC9B,UAAI,KAAKX,UAAL,CAAgBY,GAAhB,CAAoBD,IAApB,CAAJ,EAA+B;AAC7BF,QAAAA,MAAM,CAACI,IAAP,CAAYF,IAAZ;AACA;AACD;;AACD,UAAI,KAAKrB,SAAT,EAAoB;AAClBqB,QAAAA,IAAI,GAAGA,IAAI,CAACG,WAAL,EAAP;AACD;;AACD,UAAIC,KAAc,GAAG,KAArB;AACA,YAAMC,SAAmB,GAAG,EAA5B;AACA,UAAIC,KAAa,GAAG,CAApB;;AACA,aAAOA,KAAK,GAAGN,IAAI,CAACO,MAApB,EAA4B;AAC1B,YAAIC,GAAG,GAAGR,IAAI,CAACO,MAAf;AACA,YAAIE,SAAJ;;AACA,eAAOH,KAAK,GAAGE,GAAf,EAAoB;AAClB,cAAIE,MAAM,GAAGV,IAAI,CAACW,KAAL,CAAWL,KAAX,EAAkBE,GAAlB,CAAb;;AACA,cAAIF,KAAK,GAAG,CAAZ,EAAe;AACbI,YAAAA,MAAM,GAAG,OAAOA,MAAhB;AACD;;AACD,cAAI,KAAKrB,UAAL,CAAgBY,GAAhB,CAAoBS,MAApB,CAAJ,EAAiC;AAC/BD,YAAAA,SAAS,GAAGC,MAAZ;AACA;AACD;;AACDF,UAAAA,GAAG,IAAI,CAAP;AACD;;AACD,YAAIC,SAAS,KAAKf,SAAlB,EAA6B;AAC3BU,UAAAA,KAAK,GAAG,IAAR;AACA;AACD;;AACDC,QAAAA,SAAS,CAACH,IAAV,CAAeO,SAAf;AACAH,QAAAA,KAAK,GAAGE,GAAR;AACD;;AACD,UAAIJ,KAAJ,EAAW;AACTN,QAAAA,MAAM,CAACI,IAAP,CAAY,KAAKzB,YAAjB;AACD,OAFD,MAEO;AACLqB,QAAAA,MAAM,CAACI,IAAP,CAAY,GAAGG,SAAf;AACD;AACF,KArCD;AAsCA,WAAOP,MAAP;AACD;;AAEOc,EAAAA,SAAS,CAACC,KAAD,EAAwB;AACvC,UAAMzB,KAAK,GAAG,KAAKC,UAAL,CAAgBI,GAAhB,CAAoBoB,KAApB,CAAd;AACA,WAAOzB,KAAP,aAAOA,KAAP,cAAOA,KAAP,GAAgB,KAAKI,cAArB;AACD;;AAEOsB,EAAAA,SAAS,CAACC,OAAD,EAA0B;AACzC,UAAMF,KAAK,GAAG,KAAKtB,UAAL,CAAgBE,GAAhB,CAAoBsB,OAApB,CAAd;AACA,WAAOF,KAAP,aAAOA,KAAP,cAAOA,KAAP,GAAgB,KAAKpC,YAArB;AACD;AAED;AACF;AACA;AACA;AACA;AACA;;;AACSuC,EAAAA,MAAM,CAACnB,IAAD,EAAyB;AACpCA,IAAAA,IAAI,GAAG,KAAKf,cAAL,CAAoBc,QAApB,CAA6BC,IAA7B,EAAmCoB,IAAnC,CAAwC,GAAxC,CAAP;AACA,WAAO,KAAKrB,QAAL,CAAcC,IAAd,EAAoBqB,GAApB,CAAwBL,KAAK,IAAI,KAAKD,SAAL,CAAeC,KAAf,CAAjC,CAAP;AACD;AAED;AACF;AACA;AACA;AACA;AACA;;;AACSM,EAAAA,MAAM,CAACC,QAAD,EAA6B;AACxC,UAAMC,KAAe,GAAGD,QAAQ,CAACF,GAAT,CAAaH,OAAO,IAAI,KAAKD,SAAL,CAAeC,OAAf,CAAxB,CAAxB;AACA,WAAOM,KAAK,CAACJ,IAAN,CAAW,GAAX,EAAgBK,OAAhB,CAAwB,QAAxB,EAAkC,EAAlC,CAAP;AACD;;AA/H6B","sourcesContent":["/**\n * Copyright (c) Meta Platforms, Inc. and affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n *\n * @format\n */\n\nimport {BasicTokenizer} from './BasicTokenizer';\n\nexport type WordPieceTokenizerConfig = {\n  vocab: string;\n  unknownToken?: string;\n  neverSplit?: string[];\n  lowercase?: boolean;\n};\n\nexport class WordPieceTokenizer {\n  private tokenIdMap = new Map<string, number>();\n  private idTokenMap = new Map<number, string>();\n  private unknownToken: string;\n  private unknownTokenId: number = -1;\n  private neverSplit: string[];\n  private lowercase: boolean;\n  private basicTokenizer: BasicTokenizer;\n\n  /**\n   * Construct a tokenizer with a WordPieceTokenizer object.\n   *\n   * @param config a tokenizer configuration object that specify the vocabulary and special tokens, etc.\n   */\n  constructor({\n    vocab,\n    unknownToken = '[UNK]',\n    neverSplit = ['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]'],\n    lowercase = true,\n  }: WordPieceTokenizerConfig) {\n    this.unknownToken = unknownToken;\n    this.loadVocabFromString(vocab);\n    this.lowercase = lowercase;\n    this.neverSplit = [...neverSplit];\n    this.basicTokenizer = new BasicTokenizer({\n      neverSplit: this.neverSplit,\n      lowercase: this.lowercase,\n    });\n  }\n\n  private loadVocabFromString(vocab: string): void {\n    const arr = vocab.split('\\n');\n    arr.forEach((element: string, index: number) => {\n      this.tokenIdMap.set(element, index);\n      this.idTokenMap.set(index, element);\n    });\n    const unknownTokenId = this.tokenIdMap.get(this.unknownToken);\n\n    if (unknownTokenId === undefined) {\n      throw new Error('Illegal vocabulary: unknownToken is missing.');\n    } else {\n      this.unknownTokenId = unknownTokenId;\n    }\n  }\n\n  /**\n   * Tokenizes a piece of text into its word pieces.\n   * This uses a greedy longest-match-first algorithm to perform tokenization using the given vocabulary.\n   *\n   * @param text the raw input of the model\n   * @returns an array of tokens in vocabulary representing the input text.\n   */\n  public tokenize(text: string): string[] {\n    const tokens: string[] = [];\n    const words = text.split(/\\s+/);\n\n    words.forEach((word: string) => {\n      if (this.tokenIdMap.has(word)) {\n        tokens.push(word);\n        return;\n      }\n      if (this.lowercase) {\n        word = word.toLowerCase();\n      }\n      let isBad: boolean = false;\n      const subTokens: string[] = [];\n      let start: number = 0;\n      while (start < word.length) {\n        let end = word.length;\n        let curSubstr;\n        while (start < end) {\n          let substr = word.slice(start, end);\n          if (start > 0) {\n            substr = '##' + substr;\n          }\n          if (this.tokenIdMap.has(substr)) {\n            curSubstr = substr;\n            break;\n          }\n          end -= 1;\n        }\n        if (curSubstr === undefined) {\n          isBad = true;\n          break;\n        }\n        subTokens.push(curSubstr);\n        start = end;\n      }\n      if (isBad) {\n        tokens.push(this.unknownToken);\n      } else {\n        tokens.push(...subTokens);\n      }\n    });\n    return tokens;\n  }\n\n  private tokenToId(token: string): number {\n    const index = this.tokenIdMap.get(token);\n    return index ?? this.unknownTokenId;\n  }\n\n  private idToToken(tokenId: number): string {\n    const token = this.idTokenMap.get(tokenId);\n    return token ?? this.unknownToken;\n  }\n\n  /**\n   * Encode the raw input to a NLP model to an array of number, which is tensorizable.\n   *\n   * @param text The raw input of the model\n   * @returns An array of number, which can then be used to create a tensor as model input with the torch.tensor API\n   */\n  public encode(text: string): number[] {\n    text = this.basicTokenizer.tokenize(text).join(' ');\n    return this.tokenize(text).map(token => this.tokenToId(token));\n  }\n\n  /**\n   * Decode an array of tokenIds to a string using the vocabulary\n   *\n   * @param tokenIds an array of tokenIds derived from the output of model\n   * @returns a string decoded from the output of the model\n   */\n  public decode(tokenIds: number[]): string {\n    const texts: string[] = tokenIds.map(tokenId => this.idToToken(tokenId));\n    return texts.join(' ').replace(/\\s?##/g, '');\n  }\n}\n"]}